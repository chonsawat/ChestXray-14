{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2c2b0b-69a8-4da1-b8ce-948d12537f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/ChestXray-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d055006-6bc7-4b6f-ac35-7f086eb1bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa77c59-6d7a-42a8-bc16-ae6ddfc06957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataset import Dataset\n",
    "from modules.models import Model\n",
    "from modules.parser import parse_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b108be4b-2cf0-49d3-910d-9d8eba43daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_option = None # use `imagenet` or `None` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2350b9dc-b5e0-42a3-8bb2-e8ae78a5b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant variables\n",
    "NAME = \"EfficientNetB0\"\n",
    "EPOCHS = 100\n",
    "NUM_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ae610f-e6a3-4b21-8a8b-045232b74dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "def lr_schedule(epoch, learning_rate):\n",
    "    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15bba81f-6730-485f-a717-41e3cd8f9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(NAME, weight_option, fold_num):\n",
    "    model_checkpoint_callback = ModelCheckpoint(f'results/models/facal_loss/{NAME}_{weight_option}_fold_{fold_num}.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', mode=\"min\", patience=20, verbose=1)\n",
    "    reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', mode=\"min\", factor=0.5, patience=3, verbose=1)\n",
    "    lr_logging_callback = LearningRateScheduler(lr_schedule)\n",
    "    \n",
    "    return model_checkpoint_callback, early_stop_callback, reduce_lr_callback, lr_logging_callback\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87aee62-465a-4b4c-a113-6ab34e1a3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "836f764d-58ee-4bff-a2aa-26c4b671cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 3 # use values [1-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "182cc8bd-e8ab-488c-9426-b0e33e15424e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 7, 7, 1280)       4049571   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 62720)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8028288   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,107,762\n",
      "Trainable params: 12,065,739\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "5606/5606 [==============================] - 1159s 205ms/step - loss: 0.0518 - auc_1: 0.5928 - val_loss: 0.0502 - val_auc_1: 0.6744 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "5606/5606 [==============================] - 1144s 204ms/step - loss: 0.0497 - auc_1: 0.6462 - val_loss: 0.0492 - val_auc_1: 0.6881 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "5606/5606 [==============================] - 1140s 203ms/step - loss: 0.0494 - auc_1: 0.6559 - val_loss: 0.0489 - val_auc_1: 0.6884 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "5606/5606 [==============================] - 1145s 204ms/step - loss: 0.0484 - auc_1: 0.6805 - val_loss: 0.0484 - val_auc_1: 0.6996 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "5606/5606 [==============================] - 1153s 206ms/step - loss: 0.0480 - auc_1: 0.6913 - val_loss: 0.0477 - val_auc_1: 0.7116 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "5606/5606 [==============================] - 1144s 204ms/step - loss: 0.0475 - auc_1: 0.7040 - val_loss: 0.0471 - val_auc_1: 0.7252 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "5606/5606 [==============================] - 1147s 205ms/step - loss: 0.0471 - auc_1: 0.7157 - val_loss: 0.0470 - val_auc_1: 0.7232 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "5606/5606 [==============================] - 1139s 203ms/step - loss: 0.0467 - auc_1: 0.7243 - val_loss: 0.0466 - val_auc_1: 0.7366 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "5606/5606 [==============================] - 1135s 202ms/step - loss: 0.0465 - auc_1: 0.7299 - val_loss: 0.0465 - val_auc_1: 0.7369 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "5606/5606 [==============================] - 1146s 204ms/step - loss: 0.0463 - auc_1: 0.7363 - val_loss: 0.0462 - val_auc_1: 0.7466 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "5606/5606 [==============================] - 1145s 204ms/step - loss: 0.0461 - auc_1: 0.7403 - val_loss: 0.0465 - val_auc_1: 0.7389 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "5606/5606 [==============================] - 1144s 204ms/step - loss: 0.0459 - auc_1: 0.7445 - val_loss: 0.0462 - val_auc_1: 0.7487 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "5606/5606 [==============================] - 1143s 204ms/step - loss: 0.0457 - auc_1: 0.7478 - val_loss: 0.0460 - val_auc_1: 0.7549 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "5606/5606 [==============================] - 1170s 209ms/step - loss: 0.0455 - auc_1: 0.7525 - val_loss: 0.0459 - val_auc_1: 0.7507 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "5606/5606 [==============================] - 1141s 204ms/step - loss: 0.0452 - auc_1: 0.7589 - val_loss: 0.0459 - val_auc_1: 0.7557 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "5606/5606 [==============================] - 1147s 205ms/step - loss: 0.0452 - auc_1: 0.7610 - val_loss: 0.0458 - val_auc_1: 0.7516 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0449 - auc_1: 0.7657\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5606/5606 [==============================] - 1142s 204ms/step - loss: 0.0449 - auc_1: 0.7657 - val_loss: 0.0459 - val_auc_1: 0.7529 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "5606/5606 [==============================] - 1150s 205ms/step - loss: 0.0439 - auc_1: 0.7835 - val_loss: 0.0460 - val_auc_1: 0.7557 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "5606/5606 [==============================] - 1152s 205ms/step - loss: 0.0433 - auc_1: 0.7934 - val_loss: 0.0463 - val_auc_1: 0.7563 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0428 - auc_1: 0.8012\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5606/5606 [==============================] - 1149s 205ms/step - loss: 0.0428 - auc_1: 0.8012 - val_loss: 0.0466 - val_auc_1: 0.7487 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "5606/5606 [==============================] - 1150s 205ms/step - loss: 0.0419 - auc_1: 0.8168 - val_loss: 0.0472 - val_auc_1: 0.7463 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "5606/5606 [==============================] - 1150s 205ms/step - loss: 0.0413 - auc_1: 0.8226 - val_loss: 0.0478 - val_auc_1: 0.7421 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0408 - auc_1: 0.8288\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "5606/5606 [==============================] - 1170s 209ms/step - loss: 0.0408 - auc_1: 0.8288 - val_loss: 0.0482 - val_auc_1: 0.7406 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "5606/5606 [==============================] - 1140s 203ms/step - loss: 0.0400 - auc_1: 0.8384 - val_loss: 0.0495 - val_auc_1: 0.7345 - lr: 1.2500e-04\n",
      "Epoch 25/100\n",
      "5606/5606 [==============================] - 1142s 204ms/step - loss: 0.0396 - auc_1: 0.8409 - val_loss: 0.0503 - val_auc_1: 0.7328 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0391 - auc_1: 0.8458\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "5606/5606 [==============================] - 1132s 202ms/step - loss: 0.0391 - auc_1: 0.8458 - val_loss: 0.0512 - val_auc_1: 0.7301 - lr: 6.2500e-05\n",
      "Epoch 27/100\n",
      "5606/5606 [==============================] - 1137s 203ms/step - loss: 0.0387 - auc_1: 0.8507 - val_loss: 0.0519 - val_auc_1: 0.7272 - lr: 6.2500e-05\n",
      "Epoch 28/100\n",
      "5606/5606 [==============================] - 1148s 205ms/step - loss: 0.0384 - auc_1: 0.8525 - val_loss: 0.0524 - val_auc_1: 0.7250 - lr: 6.2500e-05\n",
      "Epoch 29/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0382 - auc_1: 0.8548\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "5606/5606 [==============================] - 1150s 205ms/step - loss: 0.0382 - auc_1: 0.8548 - val_loss: 0.0528 - val_auc_1: 0.7241 - lr: 3.1250e-05\n",
      "Epoch 30/100\n",
      "5606/5606 [==============================] - 1142s 204ms/step - loss: 0.0379 - auc_1: 0.8577 - val_loss: 0.0535 - val_auc_1: 0.7204 - lr: 3.1250e-05\n",
      "Epoch 31/100\n",
      "5606/5606 [==============================] - 1146s 204ms/step - loss: 0.0377 - auc_1: 0.8589 - val_loss: 0.0536 - val_auc_1: 0.7202 - lr: 3.1250e-05\n",
      "Epoch 32/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0376 - auc_1: 0.8595\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "5606/5606 [==============================] - 1154s 206ms/step - loss: 0.0376 - auc_1: 0.8595 - val_loss: 0.0541 - val_auc_1: 0.7195 - lr: 1.5625e-05\n",
      "Epoch 33/100\n",
      "5606/5606 [==============================] - 1150s 205ms/step - loss: 0.0376 - auc_1: 0.8605 - val_loss: 0.0541 - val_auc_1: 0.7186 - lr: 1.5625e-05\n",
      "Epoch 34/100\n",
      "5606/5606 [==============================] - 1155s 206ms/step - loss: 0.0374 - auc_1: 0.8605 - val_loss: 0.0541 - val_auc_1: 0.7199 - lr: 1.5625e-05\n",
      "Epoch 35/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0373 - auc_1: 0.8633\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "5606/5606 [==============================] - 1146s 204ms/step - loss: 0.0373 - auc_1: 0.8633 - val_loss: 0.0544 - val_auc_1: 0.7189 - lr: 7.8125e-06\n",
      "Epoch 36/100\n",
      "5606/5606 [==============================] - 1143s 204ms/step - loss: 0.0373 - auc_1: 0.8616 - val_loss: 0.0544 - val_auc_1: 0.7193 - lr: 7.8125e-06\n",
      "Epoch 36: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "model_checkpoint_callback, early_stop_callback, reduce_lr_callback, lr_logging_callback = get_callbacks(NAME, weight_option, fold_num)\n",
    "\n",
    "# Path for CSV\n",
    "path = os.path.join(\"results\", \"history\", \"training_with_facal_loss\", f\"{NAME}_{weight_option}\")\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# CSV Logger\n",
    "csv_logger = CSVLogger(os.path.join(path, f\"fold_{fold_num}.csv\"))\n",
    "\n",
    "# Dataset\n",
    "train_dataset, test_dataset = dataset.get_kfold(fold_num, sample=False)\n",
    "\n",
    "# Modeling\n",
    "transfer_model = tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "    include_top=False, \n",
    "    weights=weight_option,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    # apply_class_balancing=True,\n",
    "    from_logits=True,\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    transfer_model,\n",
    "    loss_function\n",
    ")\n",
    "model = model.get_model()\n",
    "model.summary()\n",
    "\n",
    "# Visualize\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1, # Show Progress Bar while Traning\n",
    "    callbacks=[model_checkpoint_callback, csv_logger, early_stop_callback, reduce_lr_callback, lr_logging_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc1a7d-2266-4906-b023-c47d29390d3f",
   "metadata": {},
   "source": [
    "## Try drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2784d97c-0f9f-4b7b-806e-a45928311d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Documations\n",
    "a module for create a model easier for experimental\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class Model:\n",
    "    \"\"\" Use for create a different transfer learning model\n",
    "    Example\n",
    "    --------\n",
    "    >>> from modules.models import Model\n",
    "    >>> transfer_model = tf.keras.applications.resnet50.ResNet50(\n",
    "        include_top=True, \n",
    "        weights=None,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    >>> model = Model(transfer_model).get_model()\n",
    "    \"\"\"\n",
    "    def __init__(self, transfer_model, loss_function):\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        self.loss = loss_function\n",
    "        self.metrics = [tf.keras.metrics.AUC(multi_label=True)]\n",
    "        self.transfer = transfer_model\n",
    "        self.model = None\n",
    "        \n",
    "    def create_model(self):\n",
    "        \"\"\" Create a Sequential of model for instances\n",
    "        Example\n",
    "        -------\n",
    "        >>> self.create_model()\n",
    "        \"\"\"        \n",
    "        sequential_list = [self.transfer]\n",
    "        sequential_list.append(tf.keras.layers.Flatten())\n",
    "        \n",
    "        sequential_list = sequential_list + [\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            \n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            \n",
    "            tf.keras.layers.Dense(15, activation='sigmoid')\n",
    "        ]\n",
    "        self.model = tf.keras.Sequential(sequential_list)\n",
    "    \n",
    "    def compile_model(self):\n",
    "        \"\"\"Compile a Sequential of model for instances\n",
    "        Example\n",
    "        -------\n",
    "        >>> self.compile_model()\n",
    "        \"\"\"        \n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss=self.loss,\n",
    "                           metrics=self.metrics)\n",
    "        \n",
    "    def get_model(self):\n",
    "        \"\"\"Create and Compile a Sequential of model for instances\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Model\n",
    "            a Sequential of model\n",
    "        \"\"\"\n",
    "        self.create_model()\n",
    "        self.compile_model()\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2ff0a57-1f32-48bc-b357-ddffb02b0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(NAME, weight_option, fold_num, path=\"/home/jovyan/ChestXray-14/run_specific_fold\"):\n",
    "    model_checkpoint_callback = ModelCheckpoint(f'{path}/{NAME}_{weight_option}_fold_{fold_num}.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', mode=\"min\", patience=20, verbose=1)\n",
    "    reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', mode=\"min\", factor=0.5, patience=3, verbose=1)\n",
    "    lr_logging_callback = LearningRateScheduler(lr_schedule)\n",
    "    \n",
    "    return model_checkpoint_callback, early_stop_callback, reduce_lr_callback, lr_logging_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f92a3c43-7e03-4b53-b02c-6e8154d04092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 7, 7, 1280)       4049571   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 62720)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               8028288   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,107,762\n",
      "Trainable params: 12,065,739\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "5606/5606 [==============================] - 1174s 208ms/step - loss: 0.0544 - auc_2: 0.5454 - val_loss: 0.0510 - val_auc_2: 0.6091 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "5606/5606 [==============================] - 1178s 210ms/step - loss: 0.0512 - auc_2: 0.5906 - val_loss: 0.0507 - val_auc_2: 0.6092 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "5606/5606 [==============================] - 1172s 209ms/step - loss: 0.0506 - auc_2: 0.6070 - val_loss: 0.0504 - val_auc_2: 0.6261 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0502 - auc_2: 0.6195 - val_loss: 0.0495 - val_auc_2: 0.6500 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "5606/5606 [==============================] - 1174s 209ms/step - loss: 0.0501 - auc_2: 0.6240 - val_loss: 0.0498 - val_auc_2: 0.6443 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "5606/5606 [==============================] - 1172s 209ms/step - loss: 0.0500 - auc_2: 0.6264 - val_loss: 0.0497 - val_auc_2: 0.6464 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "5606/5606 [==============================] - 1173s 209ms/step - loss: 0.0495 - auc_2: 0.6407 - val_loss: 0.0492 - val_auc_2: 0.6644 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "5606/5606 [==============================] - 1174s 209ms/step - loss: 0.0493 - auc_2: 0.6471 - val_loss: 0.0493 - val_auc_2: 0.6617 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "5606/5606 [==============================] - 1168s 208ms/step - loss: 0.0492 - auc_2: 0.6509 - val_loss: 0.0492 - val_auc_2: 0.6705 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "5606/5606 [==============================] - 1173s 209ms/step - loss: 0.0489 - auc_2: 0.6595 - val_loss: 0.0490 - val_auc_2: 0.6721 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "5606/5606 [==============================] - 1178s 210ms/step - loss: 0.0487 - auc_2: 0.6627 - val_loss: 0.0489 - val_auc_2: 0.6761 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "5606/5606 [==============================] - 1171s 209ms/step - loss: 0.0486 - auc_2: 0.6648 - val_loss: 0.0487 - val_auc_2: 0.6768 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "5606/5606 [==============================] - 1178s 210ms/step - loss: 0.0485 - auc_2: 0.6709 - val_loss: 0.0494 - val_auc_2: 0.6685 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "5606/5606 [==============================] - 1175s 210ms/step - loss: 0.0484 - auc_2: 0.6737 - val_loss: 0.0486 - val_auc_2: 0.6810 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "5606/5606 [==============================] - 1175s 210ms/step - loss: 0.0484 - auc_2: 0.6757 - val_loss: 0.0486 - val_auc_2: 0.6862 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "5606/5606 [==============================] - 1169s 208ms/step - loss: 0.0484 - auc_2: 0.6760 - val_loss: 0.0487 - val_auc_2: 0.6815 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0483 - auc_2: 0.6788\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5606/5606 [==============================] - 1173s 209ms/step - loss: 0.0483 - auc_2: 0.6788 - val_loss: 0.0487 - val_auc_2: 0.6852 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "5606/5606 [==============================] - 1171s 209ms/step - loss: 0.0479 - auc_2: 0.6902 - val_loss: 0.0482 - val_auc_2: 0.6973 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "5606/5606 [==============================] - 1162s 207ms/step - loss: 0.0477 - auc_2: 0.6957 - val_loss: 0.0480 - val_auc_2: 0.7005 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "5606/5606 [==============================] - 1171s 209ms/step - loss: 0.0476 - auc_2: 0.6979 - val_loss: 0.0479 - val_auc_2: 0.7027 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "5606/5606 [==============================] - 1182s 211ms/step - loss: 0.0474 - auc_2: 0.7023 - val_loss: 0.0480 - val_auc_2: 0.7040 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0473 - auc_2: 0.7050\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5606/5606 [==============================] - 1174s 209ms/step - loss: 0.0473 - auc_2: 0.7050 - val_loss: 0.0480 - val_auc_2: 0.7073 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "5606/5606 [==============================] - 1172s 209ms/step - loss: 0.0470 - auc_2: 0.7106 - val_loss: 0.0475 - val_auc_2: 0.7114 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "5606/5606 [==============================] - 1169s 208ms/step - loss: 0.0469 - auc_2: 0.7136 - val_loss: 0.0474 - val_auc_2: 0.7128 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "5606/5606 [==============================] - 1171s 209ms/step - loss: 0.0468 - auc_2: 0.7158 - val_loss: 0.0474 - val_auc_2: 0.7123 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0467 - auc_2: 0.7160 - val_loss: 0.0474 - val_auc_2: 0.7128 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0466 - auc_2: 0.7178\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "5606/5606 [==============================] - 1172s 209ms/step - loss: 0.0466 - auc_2: 0.7178 - val_loss: 0.0473 - val_auc_2: 0.7119 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "5606/5606 [==============================] - 1173s 209ms/step - loss: 0.0465 - auc_2: 0.7217 - val_loss: 0.0473 - val_auc_2: 0.7134 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "5606/5606 [==============================] - 1168s 208ms/step - loss: 0.0464 - auc_2: 0.7228 - val_loss: 0.0473 - val_auc_2: 0.7140 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "5606/5606 [==============================] - 1182s 211ms/step - loss: 0.0464 - auc_2: 0.7250 - val_loss: 0.0473 - val_auc_2: 0.7158 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "5606/5606 [==============================] - 1181s 211ms/step - loss: 0.0463 - auc_2: 0.7271 - val_loss: 0.0472 - val_auc_2: 0.7149 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "5606/5606 [==============================] - 1174s 209ms/step - loss: 0.0463 - auc_2: 0.7279 - val_loss: 0.0472 - val_auc_2: 0.7148 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0462 - auc_2: 0.7278\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "5606/5606 [==============================] - 1178s 210ms/step - loss: 0.0462 - auc_2: 0.7278 - val_loss: 0.0472 - val_auc_2: 0.7151 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "5606/5606 [==============================] - 1179s 210ms/step - loss: 0.0461 - auc_2: 0.7286 - val_loss: 0.0472 - val_auc_2: 0.7144 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0461 - auc_2: 0.7310 - val_loss: 0.0472 - val_auc_2: 0.7147 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0461 - auc_2: 0.7304\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "5606/5606 [==============================] - 1173s 209ms/step - loss: 0.0461 - auc_2: 0.7304 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 3.1250e-05\n",
      "Epoch 37/100\n",
      "5606/5606 [==============================] - 1178s 210ms/step - loss: 0.0460 - auc_2: 0.7327 - val_loss: 0.0472 - val_auc_2: 0.7157 - lr: 3.1250e-05\n",
      "Epoch 38/100\n",
      "5606/5606 [==============================] - 1174s 209ms/step - loss: 0.0460 - auc_2: 0.7341 - val_loss: 0.0472 - val_auc_2: 0.7154 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0460 - auc_2: 0.7324\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "5606/5606 [==============================] - 1178s 210ms/step - loss: 0.0460 - auc_2: 0.7324 - val_loss: 0.0472 - val_auc_2: 0.7159 - lr: 1.5625e-05\n",
      "Epoch 40/100\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0460 - auc_2: 0.7312 - val_loss: 0.0472 - val_auc_2: 0.7162 - lr: 1.5625e-05\n",
      "Epoch 41/100\n",
      "5606/5606 [==============================] - 1174s 209ms/step - loss: 0.0459 - auc_2: 0.7344 - val_loss: 0.0472 - val_auc_2: 0.7157 - lr: 1.5625e-05\n",
      "Epoch 42/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0459 - auc_2: 0.7346\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0459 - auc_2: 0.7346 - val_loss: 0.0472 - val_auc_2: 0.7159 - lr: 7.8125e-06\n",
      "Epoch 43/100\n",
      "5606/5606 [==============================] - 1171s 209ms/step - loss: 0.0459 - auc_2: 0.7351 - val_loss: 0.0472 - val_auc_2: 0.7157 - lr: 7.8125e-06\n",
      "Epoch 44/100\n",
      "5606/5606 [==============================] - 1177s 210ms/step - loss: 0.0459 - auc_2: 0.7342 - val_loss: 0.0472 - val_auc_2: 0.7160 - lr: 7.8125e-06\n",
      "Epoch 45/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0459 - auc_2: 0.7343\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0459 - auc_2: 0.7343 - val_loss: 0.0472 - val_auc_2: 0.7157 - lr: 3.9063e-06\n",
      "Epoch 46/100\n",
      "5606/5606 [==============================] - 1178s 210ms/step - loss: 0.0459 - auc_2: 0.7358 - val_loss: 0.0472 - val_auc_2: 0.7158 - lr: 3.9063e-06\n",
      "Epoch 47/100\n",
      "5606/5606 [==============================] - 1185s 211ms/step - loss: 0.0459 - auc_2: 0.7340 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 3.9063e-06\n",
      "Epoch 48/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0459 - auc_2: 0.7345\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "5606/5606 [==============================] - 1171s 209ms/step - loss: 0.0459 - auc_2: 0.7345 - val_loss: 0.0472 - val_auc_2: 0.7156 - lr: 1.9531e-06\n",
      "Epoch 49/100\n",
      "5606/5606 [==============================] - 1179s 210ms/step - loss: 0.0459 - auc_2: 0.7345 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 1.9531e-06\n",
      "Epoch 50/100\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0459 - auc_2: 0.7342 - val_loss: 0.0472 - val_auc_2: 0.7156 - lr: 1.9531e-06\n",
      "Epoch 51/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0459 - auc_2: 0.7355\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0459 - auc_2: 0.7355 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 9.7656e-07\n",
      "Epoch 52/100\n",
      "5606/5606 [==============================] - 1177s 210ms/step - loss: 0.0459 - auc_2: 0.7353 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 9.7656e-07\n",
      "Epoch 53/100\n",
      "5606/5606 [==============================] - 1174s 209ms/step - loss: 0.0459 - auc_2: 0.7357 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 9.7656e-07\n",
      "Epoch 54/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0459 - auc_2: 0.7359\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "5606/5606 [==============================] - 1153s 206ms/step - loss: 0.0459 - auc_2: 0.7359 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 4.8828e-07\n",
      "Epoch 55/100\n",
      "5606/5606 [==============================] - 1173s 209ms/step - loss: 0.0459 - auc_2: 0.7354 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 4.8828e-07\n",
      "Epoch 56/100\n",
      "5606/5606 [==============================] - 1175s 210ms/step - loss: 0.0459 - auc_2: 0.7349 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 4.8828e-07\n",
      "Epoch 57/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0459 - auc_2: 0.7359\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "5606/5606 [==============================] - 1172s 209ms/step - loss: 0.0459 - auc_2: 0.7359 - val_loss: 0.0472 - val_auc_2: 0.7154 - lr: 2.4414e-07\n",
      "Epoch 58/100\n",
      "5606/5606 [==============================] - 1176s 210ms/step - loss: 0.0459 - auc_2: 0.7346 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 2.4414e-07\n",
      "Epoch 59/100\n",
      "5606/5606 [==============================] - 1174s 209ms/step - loss: 0.0459 - auc_2: 0.7356 - val_loss: 0.0472 - val_auc_2: 0.7156 - lr: 2.4414e-07\n",
      "Epoch 60/100\n",
      "5606/5606 [==============================] - ETA: 0s - loss: 0.0459 - auc_2: 0.7356\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "5606/5606 [==============================] - 1171s 209ms/step - loss: 0.0459 - auc_2: 0.7356 - val_loss: 0.0472 - val_auc_2: 0.7154 - lr: 1.2207e-07\n",
      "Epoch 61/100\n",
      "5606/5606 [==============================] - 1172s 209ms/step - loss: 0.0459 - auc_2: 0.7340 - val_loss: 0.0472 - val_auc_2: 0.7156 - lr: 1.2207e-07\n",
      "Epoch 62/100\n",
      "5606/5606 [==============================] - 1175s 210ms/step - loss: 0.0459 - auc_2: 0.7352 - val_loss: 0.0472 - val_auc_2: 0.7155 - lr: 1.2207e-07\n",
      "Epoch 62: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Path\n",
    "path = os.path.join(\"results\", \"history\", \"training_with_facal_loss\", \"dropout_20\", f\"{NAME}_{weight_option}\")\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Callbacks\n",
    "model_checkpoint_callback, early_stop_callback, reduce_lr_callback, lr_logging_callback = get_callbacks(\n",
    "    NAME, weight_option, fold_num, path=path\n",
    ")\n",
    "\n",
    "# CSV Logger\n",
    "csv_logger = CSVLogger(os.path.join(path, f\"fold_{fold_num}.csv\"))\n",
    "\n",
    "# Dataset\n",
    "train_dataset, test_dataset = dataset.get_kfold(fold_num, sample=False)\n",
    "\n",
    "# Modeling\n",
    "transfer_model = tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "    include_top=False, \n",
    "    weights=weight_option,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "loss_function = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    # apply_class_balancing=True,\n",
    "    from_logits=True,\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    transfer_model,\n",
    "    loss_function\n",
    ")\n",
    "model = model.get_model()\n",
    "model.summary()\n",
    "\n",
    "# Visualize\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1, # Show Progress Bar while Traning\n",
    "    callbacks=[model_checkpoint_callback, csv_logger, early_stop_callback, reduce_lr_callback, lr_logging_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f81c7f-64cd-44c9-810a-3f5eebe0f8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
