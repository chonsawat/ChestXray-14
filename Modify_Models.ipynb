{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc1dacf-2dbb-46aa-931b-db6d0d83293a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0005 0.0006065306597126335\n",
      "1 0.00025 0.00036787944117144236\n",
      "2 0.000125 0.00022313016014842985\n",
      "3 6.25e-05 0.0001353352832366127\n",
      "4 3.125e-05 8.20849986238988e-05\n",
      "5 1.5625e-05 4.978706836786395e-05\n",
      "6 7.8125e-06 3.0197383422318505e-05\n",
      "7 3.90625e-06 1.8315638888734184e-05\n",
      "8 1.953125e-06 1.1108996538242308e-05\n",
      "9 9.765625e-07 6.737946999085468e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "lr_1 = 0.001\n",
    "lr_2 = 0.001\n",
    "lr_1_list = []\n",
    "lr_2_list = []\n",
    "for i in range(epoch):\n",
    "    lr_1 = lr_1 * 0.5\n",
    "    lr_2 = lr_2 * np.exp(-0.5)\n",
    "    print(i, lr_1, lr_2)\n",
    "    lr_1_list.append(lr_1)\n",
    "    lr_2_list.append(lr_2)\n",
    "\n",
    "plt.plot(range(epoch), lr_1_list, label=\"Divided by 2\")\n",
    "plt.plot(range(epoch), lr_2_list, label=\"Exponetial\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a6138-08d2-4d6e-8e62-6bc55a1031e9",
   "metadata": {},
   "source": [
    "## CSV Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14c4bab5-7d6c-4717-a71c-6c676508b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "from modules.dataset import Dataset\n",
    "from modules.models import Model\n",
    "from modules.parser import parse_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "455ba0d5-d6d6-4058-8238-b7a60ea8386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, learning_rate):\n",
    "    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78b5f648-42a4-4ef5-8030-0dcc2b0b6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option = parse_option()\n",
    "option = True\n",
    "weight_option = 'imagenet' if option else None\n",
    "\n",
    "# Constant variables\n",
    "NAME = \"Resnet50\"\n",
    "EPOCHS = 100\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# Callbacks\n",
    "model_checkpoint_callback = ModelCheckpoint(f'results/models/{NAME}.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', mode=\"min\", patience=20, verbose=1)\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', mode=\"min\", factor=0.5, patience=5, verbose=1)\n",
    "lr_logging_callback = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f7e41d2-4042-484f-a113-51295fa955d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 128)               12845184  \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,462,799\n",
      "Trainable params: 36,409,679\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 14s 182ms/step - loss: 0.3449 - auc_14: 0.5439 - val_loss: 0.4071 - val_auc_14: 0.4954 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 0.2206 - auc_14: 0.5649 - val_loss: 0.2503 - val_auc_14: 0.4394 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 0.2133 - auc_14: 0.6206 - val_loss: 0.2435 - val_auc_14: 0.5187 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.2073 - auc_14: 0.6592 - val_loss: 0.2934 - val_auc_14: 0.5325 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1969 - auc_14: 0.7155 - val_loss: 0.2556 - val_auc_14: 0.5058 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.1814 - auc_14: 0.7769 - val_loss: 0.2776 - val_auc_14: 0.5244 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1756 - auc_14: 0.8039 - val_loss: 0.2455 - val_auc_14: 0.5224 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1561 - auc_14: 0.8600\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1561 - auc_14: 0.8600 - val_loss: 0.3048 - val_auc_14: 0.5114 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1242 - auc_14: 0.9246 - val_loss: 0.3235 - val_auc_14: 0.5450 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0881 - auc_14: 0.9719 - val_loss: 0.3460 - val_auc_14: 0.5082 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0731 - auc_14: 0.9778 - val_loss: 0.3676 - val_auc_14: 0.5099 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0663 - auc_14: 0.9793 - val_loss: 0.2801 - val_auc_14: 0.5158 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0453 - auc_14: 0.9870\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0453 - auc_14: 0.9870 - val_loss: 0.3386 - val_auc_14: 0.5526 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0295 - auc_14: 0.9825 - val_loss: 0.3771 - val_auc_14: 0.5405 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 0.0170 - auc_14: 0.9981 - val_loss: 0.4536 - val_auc_14: 0.5055 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0121 - auc_14: 0.9975 - val_loss: 0.4477 - val_auc_14: 0.5137 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0082 - auc_14: 0.9984 - val_loss: 0.3951 - val_auc_14: 0.5279 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0065 - auc_14: 0.9998\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0065 - auc_14: 0.9998 - val_loss: 0.4621 - val_auc_14: 0.5294 - lr: 1.2500e-04\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0050 - auc_14: 0.9999 - val_loss: 0.4813 - val_auc_14: 0.5333 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0033 - auc_14: 1.0000 - val_loss: 0.5061 - val_auc_14: 0.5452 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0028 - auc_14: 1.0000 - val_loss: 0.5363 - val_auc_14: 0.5509 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0023 - auc_14: 1.0000 - val_loss: 0.5699 - val_auc_14: 0.5529 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0020 - auc_14: 1.0000\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0020 - auc_14: 1.0000 - val_loss: 0.5843 - val_auc_14: 0.5444 - lr: 6.2500e-05\n",
      "Epoch 23: early stopping\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 128)               12845184  \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,462,799\n",
      "Trainable params: 36,409,679\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 12s 148ms/step - loss: 0.3285 - auc_15: 0.5492 - val_loss: 19.8330 - val_auc_15: 0.4606 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.2154 - auc_15: 0.6336 - val_loss: 24.9789 - val_auc_15: 0.4667 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.2022 - auc_15: 0.6949 - val_loss: 0.2799 - val_auc_15: 0.4163 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 8s 156ms/step - loss: 0.1797 - auc_15: 0.8015 - val_loss: 0.2167 - val_auc_15: 0.4609 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1730 - auc_15: 0.8302 - val_loss: 0.4742 - val_auc_15: 0.3982 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1898 - auc_15: 0.7685 - val_loss: 0.2267 - val_auc_15: 0.4828 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.1594 - auc_15: 0.8659 - val_loss: 0.2370 - val_auc_15: 0.4391 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.1367 - auc_15: 0.9139 - val_loss: 0.3472 - val_auc_15: 0.4775 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1145 - auc_15: 0.9405\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1145 - auc_15: 0.9405 - val_loss: 0.3299 - val_auc_15: 0.4544 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0822 - auc_15: 0.9736 - val_loss: 0.4830 - val_auc_15: 0.4652 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0580 - auc_15: 0.9876 - val_loss: 0.2749 - val_auc_15: 0.4671 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0318 - auc_15: 0.9972 - val_loss: 0.2953 - val_auc_15: 0.4720 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0191 - auc_15: 0.9986 - val_loss: 0.3201 - val_auc_15: 0.4677 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0160 - auc_15: 0.9986\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0160 - auc_15: 0.9986 - val_loss: 0.4371 - val_auc_15: 0.4632 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0121 - auc_15: 0.9973 - val_loss: 0.4389 - val_auc_15: 0.4696 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0050 - auc_15: 0.9999 - val_loss: 0.4556 - val_auc_15: 0.4728 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0034 - auc_15: 1.0000 - val_loss: 0.4841 - val_auc_15: 0.4698 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0023 - auc_15: 1.0000 - val_loss: 0.5028 - val_auc_15: 0.4810 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0016 - auc_15: 1.0000\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 0.0016 - auc_15: 1.0000 - val_loss: 0.5241 - val_auc_15: 0.4948 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0014 - auc_15: 1.0000 - val_loss: 0.5336 - val_auc_15: 0.4963 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0012 - auc_15: 1.0000 - val_loss: 0.5481 - val_auc_15: 0.5052 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0011 - auc_15: 1.0000 - val_loss: 0.5683 - val_auc_15: 0.4869 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 9.7415e-04 - auc_15: 1.0000 - val_loss: 0.5940 - val_auc_15: 0.4907 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 8.9670e-04 - auc_15: 1.0000\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 8.9670e-04 - auc_15: 1.0000 - val_loss: 0.6194 - val_auc_15: 0.4930 - lr: 6.2500e-05\n",
      "Epoch 24: early stopping\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 128)               12845184  \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,462,799\n",
      "Trainable params: 36,409,679\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 12s 130ms/step - loss: 0.3577 - auc_16: 0.5384 - val_loss: 19.9324 - val_auc_16: 0.5000 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 8s 152ms/step - loss: 0.2286 - auc_16: 0.5201 - val_loss: 0.2148 - val_auc_16: 0.5411 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.2179 - auc_16: 0.5791 - val_loss: 0.3418 - val_auc_16: 0.5053 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.2141 - auc_16: 0.6274 - val_loss: 0.3115 - val_auc_16: 0.5226 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.2054 - auc_16: 0.6745 - val_loss: 0.2265 - val_auc_16: 0.5366 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1983 - auc_16: 0.7212 - val_loss: 0.3395 - val_auc_16: 0.4705 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1931 - auc_16: 0.7370\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1931 - auc_16: 0.7370 - val_loss: 0.6559 - val_auc_16: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1781 - auc_16: 0.7920 - val_loss: 0.3851 - val_auc_16: 0.5359 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1564 - auc_16: 0.8613 - val_loss: 0.3801 - val_auc_16: 0.4182 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1411 - auc_16: 0.8809 - val_loss: 0.2494 - val_auc_16: 0.5173 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1267 - auc_16: 0.9183 - val_loss: 0.4524 - val_auc_16: 0.4670 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1253 - auc_16: 0.9203\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1253 - auc_16: 0.9203 - val_loss: 0.3711 - val_auc_16: 0.5285 - lr: 2.5000e-04\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1014 - auc_16: 0.9516 - val_loss: 0.4295 - val_auc_16: 0.5393 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0767 - auc_16: 0.9728 - val_loss: 0.4378 - val_auc_16: 0.5395 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0581 - auc_16: 0.9859 - val_loss: 0.4567 - val_auc_16: 0.5481 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0473 - auc_16: 0.9895 - val_loss: 0.4116 - val_auc_16: 0.5464 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0370 - auc_16: 0.9920\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0370 - auc_16: 0.9920 - val_loss: 0.3507 - val_auc_16: 0.5810 - lr: 1.2500e-04\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0341 - auc_16: 0.9944 - val_loss: 0.3531 - val_auc_16: 0.5992 - lr: 1.2500e-04\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0226 - auc_16: 0.9979 - val_loss: 0.3603 - val_auc_16: 0.5796 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0166 - auc_16: 0.9991 - val_loss: 0.3807 - val_auc_16: 0.5916 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0124 - auc_16: 0.9986 - val_loss: 0.4085 - val_auc_16: 0.6132 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0101 - auc_16: 0.9995\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0101 - auc_16: 0.9995 - val_loss: 0.4619 - val_auc_16: 0.6225 - lr: 6.2500e-05\n",
      "Epoch 22: early stopping\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 128)               12845184  \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,462,799\n",
      "Trainable params: 36,409,679\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 11s 127ms/step - loss: 0.3205 - auc_17: 0.5365 - val_loss: 0.5421 - val_auc_17: 0.4923 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.2165 - auc_17: 0.6336 - val_loss: 177.7931 - val_auc_17: 0.5000 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1959 - auc_17: 0.7441 - val_loss: 151.4371 - val_auc_17: 0.5000 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1847 - auc_17: 0.7856 - val_loss: 0.2604 - val_auc_17: 0.4855 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1926 - auc_17: 0.7252 - val_loss: 0.3397 - val_auc_17: 0.5156 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1853 - auc_17: 0.7520 - val_loss: 0.3116 - val_auc_17: 0.5117 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1692 - auc_17: 0.8255 - val_loss: 0.2772 - val_auc_17: 0.5268 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1462 - auc_17: 0.8794 - val_loss: 0.5099 - val_auc_17: 0.5056 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1231 - auc_17: 0.9321\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.1231 - auc_17: 0.9321 - val_loss: 0.2995 - val_auc_17: 0.5250 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0959 - auc_17: 0.9533 - val_loss: 0.4263 - val_auc_17: 0.5204 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0618 - auc_17: 0.9856 - val_loss: 0.4170 - val_auc_17: 0.5276 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0436 - auc_17: 0.9809 - val_loss: 0.3191 - val_auc_17: 0.5243 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0356 - auc_17: 0.9955 - val_loss: 0.2970 - val_auc_17: 0.5259 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0375 - auc_17: 0.9889\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0375 - auc_17: 0.9889 - val_loss: 0.3665 - val_auc_17: 0.5147 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0251 - auc_17: 0.9854 - val_loss: 0.3848 - val_auc_17: 0.5023 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0131 - auc_17: 0.9991 - val_loss: 0.4141 - val_auc_17: 0.4991 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0091 - auc_17: 0.9975 - val_loss: 0.4533 - val_auc_17: 0.5019 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0076 - auc_17: 0.9994 - val_loss: 0.4449 - val_auc_17: 0.5188 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0057 - auc_17: 0.9995\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0057 - auc_17: 0.9995 - val_loss: 0.4558 - val_auc_17: 0.5251 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0042 - auc_17: 1.0000 - val_loss: 0.4679 - val_auc_17: 0.5536 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0032 - auc_17: 1.0000 - val_loss: 0.4809 - val_auc_17: 0.5524 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0026 - auc_17: 1.0000 - val_loss: 0.5051 - val_auc_17: 0.5393 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0023 - auc_17: 1.0000 - val_loss: 0.5350 - val_auc_17: 0.5220 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0023 - auc_17: 0.9997\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0023 - auc_17: 0.9997 - val_loss: 0.5685 - val_auc_17: 0.5205 - lr: 6.2500e-05\n",
      "Epoch 24: early stopping\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_18 (Flatten)        (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 128)               12845184  \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,462,799\n",
      "Trainable params: 36,409,679\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 11s 126ms/step - loss: 0.3446 - auc_18: 0.5189 - val_loss: 1.1185 - val_auc_18: 0.4667 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.2203 - auc_18: 0.6005 - val_loss: 0.2421 - val_auc_18: 0.4260 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.2140 - auc_18: 0.6383 - val_loss: 0.2510 - val_auc_18: 0.4546 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.2052 - auc_18: 0.6952 - val_loss: 0.6945 - val_auc_18: 0.4745 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1989 - auc_18: 0.7292 - val_loss: 0.2934 - val_auc_18: 0.4763 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.1923 - auc_18: 0.7613 - val_loss: 0.5876 - val_auc_18: 0.4701 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1854 - auc_18: 0.7797\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.1854 - auc_18: 0.7797 - val_loss: 0.2907 - val_auc_18: 0.4734 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.1525 - auc_18: 0.8749 - val_loss: 0.2876 - val_auc_18: 0.4706 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.1216 - auc_18: 0.9420 - val_loss: 0.2939 - val_auc_18: 0.4892 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0929 - auc_18: 0.9650 - val_loss: 0.2970 - val_auc_18: 0.4894 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0773 - auc_18: 0.9767 - val_loss: 0.2473 - val_auc_18: 0.5064 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0642 - auc_18: 0.9835\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0642 - auc_18: 0.9835 - val_loss: 0.4559 - val_auc_18: 0.4908 - lr: 2.5000e-04\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0517 - auc_18: 0.9902 - val_loss: 0.2670 - val_auc_18: 0.5100 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0285 - auc_18: 0.9972 - val_loss: 0.3511 - val_auc_18: 0.5050 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0183 - auc_18: 0.9962 - val_loss: 0.3865 - val_auc_18: 0.4931 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0149 - auc_18: 0.9989 - val_loss: 0.4022 - val_auc_18: 0.5133 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0090 - auc_18: 0.9997\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0090 - auc_18: 0.9997 - val_loss: 0.4619 - val_auc_18: 0.5019 - lr: 1.2500e-04\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0051 - auc_18: 1.0000 - val_loss: 0.5186 - val_auc_18: 0.5080 - lr: 1.2500e-04\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0029 - auc_18: 1.0000 - val_loss: 0.5603 - val_auc_18: 0.5035 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0027 - auc_18: 1.0000 - val_loss: 0.6032 - val_auc_18: 0.4984 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0018 - auc_18: 1.0000 - val_loss: 0.6302 - val_auc_18: 0.5006 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0015 - auc_18: 1.0000\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 0.0015 - auc_18: 1.0000 - val_loss: 0.6526 - val_auc_18: 0.5104 - lr: 6.2500e-05\n",
      "Epoch 22: early stopping\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "for fold_num in range(1, NUM_FOLDS + 1):\n",
    "    \n",
    "    # CSV Logger\n",
    "    path = os.path.join(\"results\", \"history\", NAME)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    csv_logger = CSVLogger(os.path.join(path, f\"fold_{fold_num}.csv\"))\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset, test_dataset = dataset.get_kfold(fold_num, sample=True)\n",
    "    \n",
    "    # Modeling\n",
    "    transfer_model = tf.keras.applications.resnet50.ResNet50(\n",
    "        include_top=False, \n",
    "        weights=weight_option,\n",
    "        input_shape=(224, 224, 3),\n",
    "        pooling=None\n",
    "    )\n",
    "    model = Model(transfer_model).get_model(flatten=True)\n",
    "    model.summary()\n",
    "    \n",
    "    # Visualize\n",
    "    history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=test_dataset,\n",
    "            verbose=1,\n",
    "            # callbacks=[WandbCallback()],\n",
    "            callbacks=[model_checkpoint_callback, csv_logger, early_stop_callback, reduce_lr_callback, lr_logging_callback]\n",
    "    )\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a105d9dc-190f-4ea1-82cd-81ed738e787e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>auc_14</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_auc_14</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.543873</td>\n",
       "      <td>0.344908</td>\n",
       "      <td>0.495432</td>\n",
       "      <td>0.407070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.564923</td>\n",
       "      <td>0.220604</td>\n",
       "      <td>0.439384</td>\n",
       "      <td>0.250341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.620595</td>\n",
       "      <td>0.213311</td>\n",
       "      <td>0.518719</td>\n",
       "      <td>0.243452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.659176</td>\n",
       "      <td>0.207349</td>\n",
       "      <td>0.532511</td>\n",
       "      <td>0.293387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.715497</td>\n",
       "      <td>0.196907</td>\n",
       "      <td>0.505843</td>\n",
       "      <td>0.255570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.776863</td>\n",
       "      <td>0.181363</td>\n",
       "      <td>0.524408</td>\n",
       "      <td>0.277555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.803859</td>\n",
       "      <td>0.175622</td>\n",
       "      <td>0.522431</td>\n",
       "      <td>0.245467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.859983</td>\n",
       "      <td>0.156107</td>\n",
       "      <td>0.511414</td>\n",
       "      <td>0.304781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.924579</td>\n",
       "      <td>0.124180</td>\n",
       "      <td>0.545029</td>\n",
       "      <td>0.323503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.971885</td>\n",
       "      <td>0.088136</td>\n",
       "      <td>0.508241</td>\n",
       "      <td>0.345958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.977790</td>\n",
       "      <td>0.073094</td>\n",
       "      <td>0.509872</td>\n",
       "      <td>0.367582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.979287</td>\n",
       "      <td>0.066307</td>\n",
       "      <td>0.515764</td>\n",
       "      <td>0.280132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.986952</td>\n",
       "      <td>0.045250</td>\n",
       "      <td>0.552597</td>\n",
       "      <td>0.338610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.982469</td>\n",
       "      <td>0.029471</td>\n",
       "      <td>0.540504</td>\n",
       "      <td>0.377116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.998081</td>\n",
       "      <td>0.016955</td>\n",
       "      <td>0.505523</td>\n",
       "      <td>0.453558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.997506</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.513722</td>\n",
       "      <td>0.447707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.998371</td>\n",
       "      <td>0.008157</td>\n",
       "      <td>0.527893</td>\n",
       "      <td>0.395148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.529427</td>\n",
       "      <td>0.462143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.533285</td>\n",
       "      <td>0.481330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.545215</td>\n",
       "      <td>0.506100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.550881</td>\n",
       "      <td>0.536305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.552851</td>\n",
       "      <td>0.569913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.544352</td>\n",
       "      <td>0.584323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch    auc_14      loss  val_auc_14  val_loss\n",
       "0       0  0.543873  0.344908    0.495432  0.407070\n",
       "1       1  0.564923  0.220604    0.439384  0.250341\n",
       "2       2  0.620595  0.213311    0.518719  0.243452\n",
       "3       3  0.659176  0.207349    0.532511  0.293387\n",
       "4       4  0.715497  0.196907    0.505843  0.255570\n",
       "5       5  0.776863  0.181363    0.524408  0.277555\n",
       "6       6  0.803859  0.175622    0.522431  0.245467\n",
       "7       7  0.859983  0.156107    0.511414  0.304781\n",
       "8       8  0.924579  0.124180    0.545029  0.323503\n",
       "9       9  0.971885  0.088136    0.508241  0.345958\n",
       "10     10  0.977790  0.073094    0.509872  0.367582\n",
       "11     11  0.979287  0.066307    0.515764  0.280132\n",
       "12     12  0.986952  0.045250    0.552597  0.338610\n",
       "13     13  0.982469  0.029471    0.540504  0.377116\n",
       "14     14  0.998081  0.016955    0.505523  0.453558\n",
       "15     15  0.997506  0.012050    0.513722  0.447707\n",
       "16     16  0.998371  0.008157    0.527893  0.395148\n",
       "17     17  0.999768  0.006453    0.529427  0.462143\n",
       "18     18  0.999892  0.004956    0.533285  0.481330\n",
       "19     19  0.999984  0.003283    0.545215  0.506100\n",
       "20     20  0.999965  0.002768    0.550881  0.536305\n",
       "21     21  0.999992  0.002320    0.552851  0.569913\n",
       "22     22  1.000000  0.002037    0.544352  0.584323"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"results/history/Resnet50/fold_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "789c8845-e304-49c5-bac7-424f2c0c7720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>auc_15</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_auc_15</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.549209</td>\n",
       "      <td>0.328484</td>\n",
       "      <td>0.460617</td>\n",
       "      <td>19.833015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.633587</td>\n",
       "      <td>0.215444</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>24.978901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.694868</td>\n",
       "      <td>0.202243</td>\n",
       "      <td>0.416347</td>\n",
       "      <td>0.279881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.801480</td>\n",
       "      <td>0.179690</td>\n",
       "      <td>0.460878</td>\n",
       "      <td>0.216734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.830194</td>\n",
       "      <td>0.173048</td>\n",
       "      <td>0.398180</td>\n",
       "      <td>0.474231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.768488</td>\n",
       "      <td>0.189831</td>\n",
       "      <td>0.482807</td>\n",
       "      <td>0.226738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.865866</td>\n",
       "      <td>0.159410</td>\n",
       "      <td>0.439065</td>\n",
       "      <td>0.237037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.913941</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.347172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.940493</td>\n",
       "      <td>0.114527</td>\n",
       "      <td>0.454428</td>\n",
       "      <td>0.329909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.973578</td>\n",
       "      <td>0.082219</td>\n",
       "      <td>0.465235</td>\n",
       "      <td>0.482997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.987618</td>\n",
       "      <td>0.057961</td>\n",
       "      <td>0.467059</td>\n",
       "      <td>0.274949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.031761</td>\n",
       "      <td>0.472046</td>\n",
       "      <td>0.295304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>0.019111</td>\n",
       "      <td>0.467690</td>\n",
       "      <td>0.320128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.998630</td>\n",
       "      <td>0.016022</td>\n",
       "      <td>0.463224</td>\n",
       "      <td>0.437081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.997337</td>\n",
       "      <td>0.012098</td>\n",
       "      <td>0.469602</td>\n",
       "      <td>0.438935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>0.472829</td>\n",
       "      <td>0.455556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>0.469845</td>\n",
       "      <td>0.484108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.480962</td>\n",
       "      <td>0.502773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.494795</td>\n",
       "      <td>0.524115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.496258</td>\n",
       "      <td>0.533608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.505187</td>\n",
       "      <td>0.548128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.486870</td>\n",
       "      <td>0.568263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.490660</td>\n",
       "      <td>0.593974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.492971</td>\n",
       "      <td>0.619354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch    auc_15      loss  val_auc_15   val_loss\n",
       "0       0  0.549209  0.328484    0.460617  19.833015\n",
       "1       1  0.633587  0.215444    0.466667  24.978901\n",
       "2       2  0.694868  0.202243    0.416347   0.279881\n",
       "3       3  0.801480  0.179690    0.460878   0.216734\n",
       "4       4  0.830194  0.173048    0.398180   0.474231\n",
       "5       5  0.768488  0.189831    0.482807   0.226738\n",
       "6       6  0.865866  0.159410    0.439065   0.237037\n",
       "7       7  0.913941  0.136700    0.477500   0.347172\n",
       "8       8  0.940493  0.114527    0.454428   0.329909\n",
       "9       9  0.973578  0.082219    0.465235   0.482997\n",
       "10     10  0.987618  0.057961    0.467059   0.274949\n",
       "11     11  0.997169  0.031761    0.472046   0.295304\n",
       "12     12  0.998643  0.019111    0.467690   0.320128\n",
       "13     13  0.998630  0.016022    0.463224   0.437081\n",
       "14     14  0.997337  0.012098    0.469602   0.438935\n",
       "15     15  0.999896  0.005007    0.472829   0.455556\n",
       "16     16  0.999985  0.003353    0.469845   0.484108\n",
       "17     17  0.999983  0.002317    0.480962   0.502773\n",
       "18     18  1.000000  0.001576    0.494795   0.524115\n",
       "19     19  1.000000  0.001358    0.496258   0.533608\n",
       "20     20  1.000000  0.001170    0.505187   0.548128\n",
       "21     21  1.000000  0.001056    0.486870   0.568263\n",
       "22     22  1.000000  0.000974    0.490660   0.593974\n",
       "23     23  1.000000  0.000897    0.492971   0.619354"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"results/history/Resnet50/fold_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fd6d3-a047-4253-a7f8-efa89f7fb70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c40668-d62c-4fae-bf40-70216ead22d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77ba29-c1de-4e01-b57b-1905c69f4cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
